#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
IngestÃ£o automÃ¡tica de notÃ­cias (texto) â€“ SeligaNerd
Ãšltima atualizaÃ§Ã£o: 24 abr 2025

â€¢ Inclui feeds brasileiros de cinema, games e quadrinhos
â€¢ Captura thumbnail (inclusive via og:image) e imagens internas (lazy-load)
â€¢ Remove itens de podcast / YouTube
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# tenta usar cloudscraper para sites protegidos por Cloudflare (ex.: CBR)
try:
    import cloudscraper
except ImportError:
    cloudscraper = None

import sys, requests, base64, json, time, csv, os, traceback, re, urllib3
from urllib.parse import urljoin
import feedparser
from requests.exceptions import RequestException
import google.generativeai as genai
from bs4 import BeautifulSoup
from datetime import datetime
import markdown
# Desabilita avisos SSL self-signed
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG GERAIS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
endpoint        = 'seliganerd.com'             # ou homolog.seliganerd.com
username        = 'gemini'
password        = 'rmjfk1exYJg^UZtunoG1&jhp'
api_key_gemini  = os.getenv("GEMINI_API_KEY", "AIzaSyAgpGU-2_76WV6-n_YLIdSvmgaIW40v-Tw")
csv_file        = 'posted_news.csv'

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GOOGLE GEMINI & GROQ SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
genai.configure(api_key=api_key_gemini)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PREFERÃŠNCIA DE MODELOS GEMINI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
PREFERRED_MODELS = [
    "gemini-1.5-pro-latest",   # mais inteligente, contexto grande
    "gemini-pro",              # 1.x estÃ¡vel
    "gemini-2.0-flash-latest"  # barato / rÃ¡pido
]


def _is_model_available(model_id: str) -> bool:
    """
    Verifica via list_models() se o identificador informado estÃ¡ liberado
    para o seu projeto e se suporta generateContent.
    """
    try:
        for mdl in genai.list_models():
            # nomes retornam como 'models/gemini-pro' etc.
            if mdl.name.endswith(model_id) and "generateContent" in mdl.supported_generation_methods:
                return True
    except Exception as e:
        # Falha de rede ou permissÃ£o â€“ assume indisponÃ­vel
        print("Aviso: nÃ£o foi possÃ­vel consultar list_models():", e)
    return False


def get_first_available_model():
    """
    Seleciona o primeiro modelo da lista PREFERRED_MODELS realmente disponÃ­vel
    para generateContent na conta. NÃ£o depende de atributos internos do SDK.
    """
    for mdl_id in PREFERRED_MODELS:
        # 1) Verifica se a API diz que o modelo existe
        if not _is_model_available(mdl_id):
            print(f"Modelo {mdl_id} indisponÃ­vel (list_models)")
            continue
        # 2) Tenta instanciar â€“ se falhar, segue para o prÃ³ximo
        try:
            m = genai.GenerativeModel(mdl_id)
            # Faz uma chamada dryâ€‘run baratÃ­ssima para garantir permissÃ£o
            _ = m.generate_content("ping").text  # usa pouquÃ­ssimos tokens
            print(f"âœ“ Gemini ativo â†’ {mdl_id}")
            return m
        except Exception as e:
            print(f"Modelo {mdl_id} indisponÃ­vel:", e)
            continue
    raise RuntimeError("Nenhum modelo Gemini disponÃ­vel para generate_content")


model = get_first_available_model()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PROMPTS PADRÃƒO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
title_prompt   = ("Traduza o seguinte tÃ­tulo de notÃ­cia para o portuguÃªs, "
                  "mantendo estilo jornalÃ­stico, conciso (mÃ¡x. 15 palavras) "
                  "e otimizado para SEO:")
excerpt_prompt = ("Escreva uma gravata (lead) em portuguÃªs, informativa e "
                  "cativante, com base no 1Âº parÃ¡grafo:")
content_prompt = ("Traduza o texto para portuguÃªs preservando estilo e tom. "
                  "Ignore tudo apÃ³s \"READ NEXT\". Divida o resultado em dois "
                  "subtÃ­tulos (<h2>):")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SELECTORES POR SITE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
site_configs = {
    # â€” sites internacionais originais â€”
    "boundingintocomics.com": {
        "content_selector": "article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "figure.tpd-post-thumbnail-container",
        "thumbnail_img_selector": "img"
    },
    "thatparkplace.com": {
        "content_selector": "div.et_pb_post_content",
        "paragraph_selector": "p",
        "img_selector": "img",
        "thumbnail_selector": "span.et_pb_image_wrap",
        "thumbnail_img_selector": "img"
    },
    "ign.com": {
        "content_selector": "article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.article-header__lead-image-wrap",
        "thumbnail_img_selector": "img"
    },
    "polygon.com": {
        "content_selector": "div.c-entry-content, div#content, article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.c-entry-hero__image-wrapper",
        "thumbnail_img_selector": "img"
    },
    "variety.com": {
        "content_selector": "div.c-content, div#article-wrapper, article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.o-article__image-container",
        "thumbnail_img_selector": "img"
    },
    "cbr.com": {
        "content_selector": "div.article-body",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.article-featured-image",
        "thumbnail_img_selector": "img"
    },
    "comicbook.com": {
        "content_selector": "div.article__content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.hero__main-image",
        "thumbnail_img_selector": "img"
    },
    # â€” NOVOS PORTAIS BRASILEIROS â€”
    "jovemnerd.com.br": {
        "content_selector": "div.post-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "figure.single__featured, div.post-thumb",
        "thumbnail_img_selector": "img"
    },
    "omelete.com.br": {
        "content_selector": "div.article-content, div.o-article__content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.o-hero--default__image, figure",
        "thumbnail_img_selector": "img"
    },
    "adorocinema.com": {
        "content_selector": "div.article-body",
        "paragraph_selector": "p",
        "img_selector": "figure, img",
        "thumbnail_selector": "div.article-poster, figure",
        "thumbnail_img_selector": "img"
    },
    "cinepop.com.br": {
        "content_selector": "div.td-post-content, div.entry-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.td-post-featured-image, figure",
        "thumbnail_img_selector": "img"
    },
    "observatoriodegames.com.br": {
        "content_selector": "div.td-post-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.td-post-featured-image",
        "thumbnail_img_selector": "img"
    },
    "observatoriodocinema.com.br": {
        "content_selector": "div.td-post-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.td-post-featured-image",
        "thumbnail_img_selector": "img"
    }
}

#
# -----------------------------------------------------------------------------
# GET com fallback para Cloudflare (cbr.com) ----------------------------------
def _http_get(url, **kwargs):
    """
    Wrapper para requests.get que usa cloudscraper em sites protegidos.
    """
    if 'cbr.com' in url and cloudscraper:
        # cloudscraper jÃ¡ inclui userâ€‘agent e cookies vÃ¡lidos
        return cloudscraper.create_scraper().get(url, **kwargs, verify=False)
    # garante Userâ€‘Agent para todos os outros
    headers = kwargs.pop('headers', {})
    headers.setdefault('User-Agent',
                       'Mozilla/5.0 (Macintosh; Intel Mac OS X 15_5_0) '
                       'AppleWebKit/537.36 (KHTML, like Gecko) '
                       'Chrome/124.0.0.0 Safari/537.36')
    return requests.get(url, headers=headers, **kwargs, verify=False)
# -----------------------------------------------------------------------------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FUNÃ‡Ã•ES AUXILIARES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def build_auth_header():
    basic = base64.b64encode(f"{username}:{password}".encode()).decode()
    return {'Authorization': f'Basic {basic}'}

def get_site_domain(url):
    from urllib.parse import urlparse
    parts = urlparse(url).netloc.split('.')
    return '.'.join(parts[-2:]) if len(parts) >= 2 else urlparse(url).netloc


def get_config_for_site(url):
    domain = get_site_domain(url)
    for site_dom, cfg in site_configs.items():
        if site_dom in domain:
            return cfg
    # fallback
    return {
        "content_selector": "article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "figure",
        "thumbnail_img_selector": "img"
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LIMPA TÃTULOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
_SITE_BRAND_NAMES = [
    "Polygon", "IGN", "Variety", "CBR", "ComicBook.com", "Bounding Into Comics",
    "That Park Place", "Jovem Nerd", "Omelete", "AdoroCinema", "CinePOP",
    "ObservatÃ³rio de Games", "ObservatÃ³rio do Cinema"
]

def clean_site_name_from_title(title:str) -> str:
    """
    Remove ocorrÃªncias do nome do site no tÃ­tulo, como:
    'TÃ­tulo da MatÃ©ria - Polygon' ou 'TÃ­tulo | IGN'.
    """
    for brand in _SITE_BRAND_NAMES:
        # remove no final: ' - Brand', ' | Brand', ' â€” Brand', etc.
        patt_end = rf"\s*[\-|â€“â€”\|:]\s*{re.escape(brand)}(\.com\.br|\.com)?\s*$"
        title = re.sub(patt_end, "", title, flags=re.I).strip()
        # remove no inÃ­cio: 'Brand: TÃ­tulo', 'Brand - TÃ­tulo'
        patt_start = rf"^{re.escape(brand)}(\.com\.br|\.com)?\s*[\-|â€“â€”:\|]\s*"
        title = re.sub(patt_start, "", title, flags=re.I).strip()
    return title


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRADUÃ‡ÃƒO AI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# Flag global para sinalizar que a cota diÃ¡ria do Gemini foi esgotada
QUOTA_EXHAUSTED = False

def translate_content(text, prompt, _attempt=0):
    """Traduz ou reescreve o texto usando Gemini, com fallback de modelo e backâ€‘off."""
    global QUOTA_EXHAUSTED
    if QUOTA_EXHAUSTED:
        # Evita chamadas desnecessÃ¡rias quando a cota jÃ¡ acabou
        return text
    ask = f"{prompt}\n\n{text}"
    try:
        rsp = model.generate_content(ask)
        if rsp and getattr(rsp, "text", None):
            return rsp.text
    except Exception as e:
        msg = str(e).lower()
        # troca de modelo se o identificador nÃ£o existir mais
        if ("404" in msg or "not found" in msg) and _attempt < len(PREFERRED_MODELS):
            print("ðŸ”„ modelo 404 â€“ trocando de modeloâ€¦")
            globals()["model"] = get_first_available_model()
            return translate_content(text, prompt, _attempt + 1)
        # Se excedeu a cota diÃ¡ria, marca flag global e devolve o texto original
        if "quota" in msg:
            print("âš ï¸  Cota diÃ¡ria do Gemini esgotada â€“ pulando traduÃ§Ãµes restantes.")
            QUOTA_EXHAUSTED = True
            return text
        # Rateâ€‘limit temporÃ¡rio â†’ backâ€‘off curto (mÃ¡x. 2 tentativas)
        if "rate limit" in msg and _attempt < 2:
            wait = 5 * (_attempt + 1)
            print(f"â³ rateâ€‘limit â€“ aguardando {wait}sâ€¦")
            time.sleep(wait)
            return translate_content(text, prompt, _attempt + 1)
        print("âš ï¸  Gemini falhou â€“ devolvendo original", e)
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTML UTILS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def text_to_html_paragraphs(text):
    style = "font-weight: 400;"
    lines = [markdown.markdown(l).strip() for l in text.split('\n') if l.strip()]
    return ''.join(f'<p style="{style}">{l}</p><br />' for l in lines)

def insere_imagens_apos_h2(html: str, ids_imgs: list[int]) -> str:
    ids_iter = iter(ids_imgs)
    def repl(match):
        h2 = match.group(0)
        try:
            img_id = next(ids_iter)
            return f'{h2}<img src="{get_img_url_by_id(img_id)}" alt="" />'
        except StopIteration:
            return h2
    return re.sub(r'(<h2>.*?</h2>)', repl, html, flags=re.I|re.S)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WORDPRESS API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def download_image(url, dest):
    # pula placeholders SVG ou data URI
    if not url or url.startswith("data:"):
        return
    try:
        r = requests.get(url, stream=True, timeout=30, verify=False)
        if r.status_code == 200:
            with open(dest, 'wb') as f:
                for chunk in r.iter_content(1024):
                    f.write(chunk)
    except Exception as e:
        print("Erro download img:", e)

def upload_image(path) -> int|None:
    try:
        with open(path, 'rb') as fp:
            content = fp.read()
        hdrs = {**build_auth_header(),
                'Content-Disposition': f'attachment; filename={os.path.basename(path)}',
                'Content-Type': 'image/jpeg'}
        resp = requests.post(f'https://{endpoint}/wp-json/wp/v2/media',
                             headers=hdrs, data=content, verify=False)
        return resp.json()['id'] if resp.status_code == 201 else None
    except Exception:
        return None

def get_img_url_by_id(media_id:int):
    resp = requests.get(f'https://{endpoint}/wp-json/wp/v2/media/{media_id}',
                        headers=build_auth_header(), verify=False)
    return resp.json().get("guid", {}).get("rendered", "#")

def get_or_create_exact_tag_id(tag_name):
    hdr = build_auth_header()
    r = requests.get(f'https://{endpoint}/wp-json/wp/v2/tags?search={tag_name}',
                     headers=hdr, verify=False)
    if r.status_code == 200:
        for t in r.json():
            if t['name'] == tag_name:
                return t['id']
    # cria
    r = requests.post(f'https://{endpoint}/wp-json/wp/v2/tags',
                      headers=hdr, json={'name': tag_name}, verify=False)
    return r.json()['id'] if r.status_code == 201 else None

def get_all_categories():
    r = requests.get(f'https://{endpoint}/wp-json/wp/v2/categories?per_page=100',
                     headers=build_auth_header(), verify=False)
    return {c['name'].lower(): c['id'] for c in r.json()}

def match_categories(text, cat_map):
    txt = text.lower()
    return [id_ for name,id_ in cat_map.items() if name in txt]

def get_or_create_category_id(name):
    hdr = build_auth_header()
    r = requests.get(f'https://{endpoint}/wp-json/wp/v2/categories?search={name}',
                     headers=hdr, verify=False)
    if r.status_code == 200:
        for c in r.json():
            if c['name'].lower() == name.lower():
                return c['id']
    r = requests.post(f'https://{endpoint}/wp-json/wp/v2/categories',
                      headers=hdr, json={'name': name}, verify=False)
    return r.json()['id']

def post_to_wordpress(title, content_html, excerpt, thumb_path, cat_ids):
    img_id = upload_image(thumb_path)
    tag_dest = get_or_create_exact_tag_id("Destaque")
    hdr = {**build_auth_header(), 'Content-Type': 'application/json'}
    data = {
        'title': title.rstrip(".").replace('**',''),
        'status': 'draft',
        'content': content_html,
        'excerpt': excerpt.replace('**',''),
        'featured_media': img_id,
        'categories': cat_ids,
        'tags': [tag_dest]
    }
    r = requests.post(f'https://{endpoint}/wp-json/wp/v2/posts',
                      headers=hdr, json=data, verify=False)
    return r.status_code == 201

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSV LOG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def is_url_posted(url):
    if not os.path.isfile(csv_file):
        return False
    with open(csv_file) as fp:
        return any(row['url'] == url for row in csv.DictReader(fp))

def log_posted(url):
    exists = os.path.isfile(csv_file)
    with open(csv_file, 'a', newline='') as fp:
        fieldnames = ['url','date','time']
        wr = csv.DictWriter(fp, fieldnames)
        if not exists:
            wr.writeheader()
        now = datetime.now()
        wr.writerow({'url':url,'date':now.strftime('%Y-%m-%d'),
                     'time':now.strftime('%H:%M:%S')})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CORE: CAPTURA DE CONTEÃšDO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def get_news_content(news_url):
    try:
        r = _http_get(news_url, timeout=30)
    except RequestException as e:
        # erro de rede ou desconexÃ£o â€” sinaliza ao chamador
        raise Exception("falha de conexÃ£o") from e

    if r.status_code != 200:
        raise Exception(f"status {r.status_code}")
    soup = BeautifulSoup(r.text, 'html.parser')
    title = soup.title.text.strip()

    cfg = get_config_for_site(news_url)
    # localiza content_div
    selector = cfg["content_selector"].split(',')
    content_div = None
    for sel in selector:
        typ = sel.strip().split('.')[0]
        cls = sel.strip().split('.')[1] if '.' in sel else None
        content_div = soup.find(typ, class_=cls) if cls else soup.find(typ)
        if content_div: break
    if not content_div:
        # tentativa genÃ©rica antes de desistir
        content_div = soup.find('article') or soup.find('main') or soup

    paragraphs = content_div.find_all(cfg["paragraph_selector"]) or content_div.find_all("p")
    if not paragraphs:
        raise Exception("conteÃºdo nÃ£o encontrado (parÃ¡grafos vazios)")
    # imgs internas
    imgs = (content_div.find_all("figure")
            if cfg["img_selector"] == "figure"
            else content_div.find_all("img"))
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ thumbnail
    thumb = None
    for sel in cfg["thumbnail_selector"].split(','):
        typ = sel.strip().split('.')[0]
        cls = sel.strip().split('.')[1] if '.' in sel else None
        thumb = soup.find(typ, class_=cls) if cls else soup.find(typ)
        if thumb: break
    if thumb and cfg["thumbnail_img_selector"] == "img":
        thumb_img = thumb.find("img")
    else:
        thumb_img = thumb
    thumbnail_url = (thumb_img.get('src') or thumb_img.get('data-src')
                     or thumb_img.get('data-lazy-src')) if thumb_img else None
    # ignora thumbnails data-URI
    if thumbnail_url and thumbnail_url.startswith("data:"):
        thumbnail_url = None
    # fallback og:image
    if not thumbnail_url:
        og = soup.find("meta", property="og:image")
        thumbnail_url = og.get("content") if og else None
        if not thumbnail_url:
            # primeira imagem externa encontrada
            for im in soup.find_all("img"):
                cand = im.get('src') or im.get('data-src') or im.get('data-lazy-src')
                if cand and cand.startswith("http"):
                    thumbnail_url = cand
                    break
    # garante que a thumbnail Ã© URL absoluta
    if thumbnail_url:
        thumbnail_url = urljoin(news_url, thumbnail_url)
    if not thumbnail_url:
        raise Exception("thumbnail nÃ£o encontrada")

    # texto
    first_idx = next((i for i,p in enumerate(paragraphs) if p.text.strip()),0)
    excerpt = paragraphs[first_idx].text.strip()
    content = '\n'.join(p.text for p in paragraphs[first_idx:]
                        if not re.search(r'RELATED|READ\s+MORE|NEXT', p.text, re.I))

    # processa imagens
    uploads = []
    for el in imgs:
        img_tag = el.find('img') if el.name == 'figure' else el
        src = (img_tag.get('src') or img_tag.get('data-src')
               or img_tag.get('data-lazy-src')) if img_tag else None
        # converte para URL absoluta
        if src:
            src = urljoin(news_url, src)
        if not src or src == thumbnail_url:
            continue
        fname = os.path.basename(src.split('?')[0])
        download_image(src, fname)
        up_id = upload_image(fname)
        if up_id: uploads.append(up_id)

    return title, excerpt, content, thumbnail_url, uploads

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LISTA DE FEEDS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
rss_feed_urls = [
    # internacionais
    "https://boundingintocomics.com/feed/",
    "https://thatparkplace.com/feed/",
    "http://feeds.ign.com/ign/all",
    "https://www.polygon.com/rss/index.xml",
    "https://www.cbr.com/feed/",
    # brasileiros â€“ cinema, games e HQ
    "https://jovemnerd.com.br/feed/",
    "https://www.omelete.com.br/rss.xml",
    "https://www.adorocinema.com/rss/noticias.xml",
    "https://cinepop.com.br/feed/",
    "https://observatoriodegames.uol.com.br/feed"
]

# URLs que nÃ£o possuem corpo de artigo (listas de compras, prÃ©-venda, promoÃ§Ãµes etc.)
IGNORE_URL_PATTERNS = [
    r"/pre-order/",
    r"/deal[s]?/",
    r"/deals?/",
    r"/list/",
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOOP PRINCIPAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
categories = get_all_categories()

def process_news_feed(feed_url):
    try:
        feed = feedparser.parse(
            _http_get(feed_url, timeout=30).content
        )
        for entry in feed.entries:
            news_url = entry.link
            # â”€â”€ DEBUG: explica por que determinados links sÃ£o ignorados â”€â”€ #
            reason = None
            if re.search(r"(podcast|nerdcast|youtube\.com|/video/|/videos?/)", news_url, re.I):
                reason = "podcast/vÃ­deo"
            elif any(re.search(pat, news_url, re.I) for pat in IGNORE_URL_PATTERNS):
                reason = "prÃ©â€‘venda/lista"
            elif is_url_posted(news_url):
                reason = "jÃ¡ postado"
            if reason:
                print("Â· SKIP", reason, "â€”", news_url)
                continue
            print("â†’", news_url)
            try:
                title, excerpt, content, thumb_url, imgs_ids = get_news_content(news_url)
                title = clean_site_name_from_title(title)

                tr_title   = translate_content(title,   title_prompt)
                tr_excerpt = translate_content(excerpt, excerpt_prompt)
                tr_content = translate_content(content, content_prompt)
                html_body  = text_to_html_paragraphs(tr_content)
                html_body  = insere_imagens_apos_h2(html_body, imgs_ids)

                # thumbnail tmp file
                thumb_path = "thumb_tmp.jpg"
                download_image(thumb_url, thumb_path)

                cat_ids = match_categories(tr_title + " " + tr_content, categories)
                if not cat_ids:
                    cat_ids = [get_or_create_category_id("NotÃ­cias")]

                if post_to_wordpress(tr_title, html_body, tr_excerpt,
                                     thumb_path, cat_ids):
                    log_posted(news_url)
            except Exception as e:
                msg = str(e).lower()
                if "conteÃºdo nÃ£o encontrado" in msg:
                    # pÃ¡gina sem corpo de artigo â€” ignora em silÃªncio
                    print("ConteÃºdo nÃ£o encontrado â€“ pulando.")
                elif "rate limit" in msg:
                    # excedeu cota da API â€” apenas registra e segue
                    print("Rate-limit da API â€“ pulando este item.")
                elif "falha de conexÃ£o" in msg:
                    print("Falha de conexÃ£o â€“ pulando este item.")
                else:
                    print("Erro item:", e)
                    traceback.print_exc()
    except Exception as e:
        print("Erro feed:", feed_url, e)

for url in rss_feed_urls:
    print(f"\n=== PROCESSANDO FEED: {url} ===")
    process_news_feed(url)
