#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ingestão automática de notícias (texto) – SeligaNerd
Última atualização: 24 abr 2025

• Inclui feeds brasileiros de cinema, games e quadrinhos
• Captura thumbnail (inclusive via og:image) e imagens internas (lazy-load)
• Remove itens de podcast / YouTube
"""

# ───────────────────────────────── IMPORTS ──────────────────────────────── #
# tenta usar cloudscraper para sites protegidos por Cloudflare (ex.: CBR)
try:
    import cloudscraper
except ImportError:
    cloudscraper = None

import sys, requests, base64, json, time, csv, os, traceback, re, urllib3
from urllib.parse import urljoin
import feedparser
from requests.exceptions import RequestException
import google.generativeai as genai
from bs4 import BeautifulSoup
from datetime import datetime
import markdown
# Desabilita avisos SSL self-signed
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ───────────────────────────── CONFIG GERAIS ───────────────────────────── #
endpoint        = 'seliganerd.com'             # ou homolog.seliganerd.com
username        = 'gemini'
password        = 'rmjfk1exYJg^UZtunoG1&jhp'
api_key_gemini  = os.getenv("GEMINI_API_KEY", "AIzaSyAgpGU-2_76WV6-n_YLIdSvmgaIW40v-Tw")
csv_file        = 'posted_news.csv'

# ─────────────────────── GOOGLE GEMINI & GROQ SETUP ────────────────────── #
genai.configure(api_key=api_key_gemini)

# ─────────────────────── PREFERÊNCIA DE MODELOS GEMINI ─────────────────────── #
PREFERRED_MODELS = [
    "gemini-1.5-pro-latest",   # mais inteligente, contexto grande
    "gemini-pro",              # 1.x estável
    "gemini-2.0-flash-latest"  # barato / rápido
]


def _is_model_available(model_id: str) -> bool:
    """
    Verifica via list_models() se o identificador informado está liberado
    para o seu projeto e se suporta generateContent.
    """
    try:
        for mdl in genai.list_models():
            # nomes retornam como 'models/gemini-pro' etc.
            if mdl.name.endswith(model_id) and "generateContent" in mdl.supported_generation_methods:
                return True
    except Exception as e:
        # Falha de rede ou permissão – assume indisponível
        print("Aviso: não foi possível consultar list_models():", e)
    return False


def get_first_available_model():
    """
    Seleciona o primeiro modelo da lista PREFERRED_MODELS realmente disponível
    para generateContent na conta. Não depende de atributos internos do SDK.
    """
    for mdl_id in PREFERRED_MODELS:
        # 1) Verifica se a API diz que o modelo existe
        if not _is_model_available(mdl_id):
            print(f"Modelo {mdl_id} indisponível (list_models)")
            continue
        # 2) Tenta instanciar – se falhar, segue para o próximo
        try:
            m = genai.GenerativeModel(mdl_id)
            # Faz uma chamada dry‑run baratíssima para garantir permissão
            _ = m.generate_content("ping").text  # usa pouquíssimos tokens
            print(f"✓ Gemini ativo → {mdl_id}")
            return m
        except Exception as e:
            print(f"Modelo {mdl_id} indisponível:", e)
            continue
    raise RuntimeError("Nenhum modelo Gemini disponível para generate_content")


model = get_first_available_model()

# ───────────────────────────── PROMPTS PADRÃO ──────────────────────────── #
title_prompt   = ("Traduza o seguinte título de notícia para o português, "
                  "mantendo estilo jornalístico, conciso (máx. 15 palavras) "
                  "e otimizado para SEO:")
excerpt_prompt = ("Escreva uma gravata (lead) em português, informativa e "
                  "cativante, com base no 1º parágrafo:")
content_prompt = ("Traduza o texto para português preservando estilo e tom. "
                  "Ignore tudo após \"READ NEXT\". Divida o resultado em dois "
                  "subtítulos (<h2>):")

# ───────────────────────────── SELECTORES POR SITE ─────────────────────── #
site_configs = {
    # — sites internacionais originais —
    "boundingintocomics.com": {
        "content_selector": "article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "figure.tpd-post-thumbnail-container",
        "thumbnail_img_selector": "img"
    },
    "thatparkplace.com": {
        "content_selector": "div.et_pb_post_content",
        "paragraph_selector": "p",
        "img_selector": "img",
        "thumbnail_selector": "span.et_pb_image_wrap",
        "thumbnail_img_selector": "img"
    },
    "ign.com": {
        "content_selector": "article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.article-header__lead-image-wrap",
        "thumbnail_img_selector": "img"
    },
    "polygon.com": {
        "content_selector": "div.c-entry-content, div#content, article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.c-entry-hero__image-wrapper",
        "thumbnail_img_selector": "img"
    },
    "variety.com": {
        "content_selector": "div.c-content, div#article-wrapper, article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.o-article__image-container",
        "thumbnail_img_selector": "img"
    },
    "cbr.com": {
        "content_selector": "div.article-body",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.article-featured-image",
        "thumbnail_img_selector": "img"
    },
    "comicbook.com": {
        "content_selector": "div.article__content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.hero__main-image",
        "thumbnail_img_selector": "img"
    },
    # — NOVOS PORTAIS BRASILEIROS —
    "jovemnerd.com.br": {
        "content_selector": "div.post-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "figure.single__featured, div.post-thumb",
        "thumbnail_img_selector": "img"
    },
    "omelete.com.br": {
        "content_selector": "div.article-content, div.o-article__content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.o-hero--default__image, figure",
        "thumbnail_img_selector": "img"
    },
    "adorocinema.com": {
        "content_selector": "div.article-body",
        "paragraph_selector": "p",
        "img_selector": "figure, img",
        "thumbnail_selector": "div.article-poster, figure",
        "thumbnail_img_selector": "img"
    },
    "cinepop.com.br": {
        "content_selector": "div.td-post-content, div.entry-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.td-post-featured-image, figure",
        "thumbnail_img_selector": "img"
    },
    "observatoriodegames.com.br": {
        "content_selector": "div.td-post-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.td-post-featured-image",
        "thumbnail_img_selector": "img"
    },
    "observatoriodocinema.com.br": {
        "content_selector": "div.td-post-content",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "div.td-post-featured-image",
        "thumbnail_img_selector": "img"
    }
}

#
# -----------------------------------------------------------------------------
# GET com fallback para Cloudflare (cbr.com) ----------------------------------
def _http_get(url, **kwargs):
    """
    Wrapper para requests.get que usa cloudscraper em sites protegidos.
    """
    if 'cbr.com' in url and cloudscraper:
        # cloudscraper já inclui user‑agent e cookies válidos
        return cloudscraper.create_scraper().get(url, **kwargs, verify=False)
    # garante User‑Agent para todos os outros
    headers = kwargs.pop('headers', {})
    headers.setdefault('User-Agent',
                       'Mozilla/5.0 (Macintosh; Intel Mac OS X 15_5_0) '
                       'AppleWebKit/537.36 (KHTML, like Gecko) '
                       'Chrome/124.0.0.0 Safari/537.36')
    return requests.get(url, headers=headers, **kwargs, verify=False)
# -----------------------------------------------------------------------------

# ───────────────────────────── FUNÇÕES AUXILIARES ──────────────────────── #
def build_auth_header():
    basic = base64.b64encode(f"{username}:{password}".encode()).decode()
    return {'Authorization': f'Basic {basic}'}

def get_site_domain(url):
    from urllib.parse import urlparse
    parts = urlparse(url).netloc.split('.')
    return '.'.join(parts[-2:]) if len(parts) >= 2 else urlparse(url).netloc


def get_config_for_site(url):
    domain = get_site_domain(url)
    for site_dom, cfg in site_configs.items():
        if site_dom in domain:
            return cfg
    # fallback
    return {
        "content_selector": "article",
        "paragraph_selector": "p",
        "img_selector": "figure",
        "thumbnail_selector": "figure",
        "thumbnail_img_selector": "img"
    }


# ───────────────────────────── LIMPA TÍTULOS ──────────────────────────── #
_SITE_BRAND_NAMES = [
    "Polygon", "IGN", "Variety", "CBR", "ComicBook.com", "Bounding Into Comics",
    "That Park Place", "Jovem Nerd", "Omelete", "AdoroCinema", "CinePOP",
    "Observatório de Games", "Observatório do Cinema"
]

def clean_site_name_from_title(title:str) -> str:
    """
    Remove ocorrências do nome do site no título, como:
    'Título da Matéria - Polygon' ou 'Título | IGN'.
    """
    for brand in _SITE_BRAND_NAMES:
        # remove no final: ' - Brand', ' | Brand', ' — Brand', etc.
        patt_end = rf"\s*[\-|–—\|:]\s*{re.escape(brand)}(\.com\.br|\.com)?\s*$"
        title = re.sub(patt_end, "", title, flags=re.I).strip()
        # remove no início: 'Brand: Título', 'Brand - Título'
        patt_start = rf"^{re.escape(brand)}(\.com\.br|\.com)?\s*[\-|–—:\|]\s*"
        title = re.sub(patt_start, "", title, flags=re.I).strip()
    return title


# ───────────────────────────── TRADUÇÃO AI ─────────────────────────────── #
# Flag global para sinalizar que a cota diária do Gemini foi esgotada
QUOTA_EXHAUSTED = False

def translate_content(text, prompt, _attempt=0):
    """Traduz ou reescreve o texto usando Gemini, com fallback de modelo e back‑off."""
    global QUOTA_EXHAUSTED
    if QUOTA_EXHAUSTED:
        # Evita chamadas desnecessárias quando a cota já acabou
        return text
    ask = f"{prompt}\n\n{text}"
    try:
        rsp = model.generate_content(ask)
        if rsp and getattr(rsp, "text", None):
            return rsp.text
    except Exception as e:
        msg = str(e).lower()
        # troca de modelo se o identificador não existir mais
        if ("404" in msg or "not found" in msg) and _attempt < len(PREFERRED_MODELS):
            print("🔄 modelo 404 – trocando de modelo…")
            globals()["model"] = get_first_available_model()
            return translate_content(text, prompt, _attempt + 1)
        # Se excedeu a cota diária, marca flag global e devolve o texto original
        if "quota" in msg:
            print("⚠️  Cota diária do Gemini esgotada – pulando traduções restantes.")
            QUOTA_EXHAUSTED = True
            return text
        # Rate‑limit temporário → back‑off curto (máx. 2 tentativas)
        if "rate limit" in msg and _attempt < 2:
            wait = 5 * (_attempt + 1)
            print(f"⏳ rate‑limit – aguardando {wait}s…")
            time.sleep(wait)
            return translate_content(text, prompt, _attempt + 1)
        print("⚠️  Gemini falhou – devolvendo original", e)
    return text

# ───────────────────────────── HTML UTILS ──────────────────────────────── #
def text_to_html_paragraphs(text):
    style = "font-weight: 400;"
    lines = [markdown.markdown(l).strip() for l in text.split('\n') if l.strip()]
    return ''.join(f'<p style="{style}">{l}</p><br />' for l in lines)

def insere_imagens_apos_h2(html: str, ids_imgs: list[int]) -> str:
    ids_iter = iter(ids_imgs)
    def repl(match):
        h2 = match.group(0)
        try:
            img_id = next(ids_iter)
            return f'{h2}<img src="{get_img_url_by_id(img_id)}" alt="" />'
        except StopIteration:
            return h2
    return re.sub(r'(<h2>.*?</h2>)', repl, html, flags=re.I|re.S)

# ───────────────────────────── WORDPRESS API ───────────────────────────── #
def download_image(url, dest):
    # pula placeholders SVG ou data URI
    if not url or url.startswith("data:"):
        return
    try:
        r = requests.get(url, stream=True, timeout=30, verify=False)
        if r.status_code == 200:
            with open(dest, 'wb') as f:
                for chunk in r.iter_content(1024):
                    f.write(chunk)
    except Exception as e:
        print("Erro download img:", e)

def upload_image(path) -> int|None:
    try:
        with open(path, 'rb') as fp:
            content = fp.read()
        hdrs = {**build_auth_header(),
                'Content-Disposition': f'attachment; filename={os.path.basename(path)}',
                'Content-Type': 'image/jpeg'}
        resp = requests.post(f'https://{endpoint}/wp-json/wp/v2/media',
                             headers=hdrs, data=content, verify=False)
        return resp.json()['id'] if resp.status_code == 201 else None
    except Exception:
        return None

def get_img_url_by_id(media_id:int):
    resp = requests.get(f'https://{endpoint}/wp-json/wp/v2/media/{media_id}',
                        headers=build_auth_header(), verify=False)
    return resp.json().get("guid", {}).get("rendered", "#")

def get_or_create_exact_tag_id(tag_name):
    hdr = build_auth_header()
    r = requests.get(f'https://{endpoint}/wp-json/wp/v2/tags?search={tag_name}',
                     headers=hdr, verify=False)
    if r.status_code == 200:
        for t in r.json():
            if t['name'] == tag_name:
                return t['id']
    # cria
    r = requests.post(f'https://{endpoint}/wp-json/wp/v2/tags',
                      headers=hdr, json={'name': tag_name}, verify=False)
    return r.json()['id'] if r.status_code == 201 else None

def get_all_categories():
    r = requests.get(f'https://{endpoint}/wp-json/wp/v2/categories?per_page=100',
                     headers=build_auth_header(), verify=False)
    return {c['name'].lower(): c['id'] for c in r.json()}

def match_categories(text, cat_map):
    txt = text.lower()
    return [id_ for name,id_ in cat_map.items() if name in txt]

def get_or_create_category_id(name):
    hdr = build_auth_header()
    r = requests.get(f'https://{endpoint}/wp-json/wp/v2/categories?search={name}',
                     headers=hdr, verify=False)
    if r.status_code == 200:
        for c in r.json():
            if c['name'].lower() == name.lower():
                return c['id']
    r = requests.post(f'https://{endpoint}/wp-json/wp/v2/categories',
                      headers=hdr, json={'name': name}, verify=False)
    return r.json()['id']

def post_to_wordpress(title, content_html, excerpt, thumb_path, cat_ids):
    img_id = upload_image(thumb_path)
    tag_dest = get_or_create_exact_tag_id("Destaque")
    hdr = {**build_auth_header(), 'Content-Type': 'application/json'}
    data = {
        'title': title.rstrip(".").replace('**',''),
        'status': 'draft',
        'content': content_html,
        'excerpt': excerpt.replace('**',''),
        'featured_media': img_id,
        'categories': cat_ids,
        'tags': [tag_dest]
    }
    r = requests.post(f'https://{endpoint}/wp-json/wp/v2/posts',
                      headers=hdr, json=data, verify=False)
    return r.status_code == 201

# ───────────────────────────── CSV LOG ─────────────────────────────────── #
def is_url_posted(url):
    if not os.path.isfile(csv_file):
        return False
    with open(csv_file) as fp:
        return any(row['url'] == url for row in csv.DictReader(fp))

def log_posted(url):
    exists = os.path.isfile(csv_file)
    with open(csv_file, 'a', newline='') as fp:
        fieldnames = ['url','date','time']
        wr = csv.DictWriter(fp, fieldnames)
        if not exists:
            wr.writeheader()
        now = datetime.now()
        wr.writerow({'url':url,'date':now.strftime('%Y-%m-%d'),
                     'time':now.strftime('%H:%M:%S')})

# ───────────────────────────── CORE: CAPTURA DE CONTEÚDO ───────────────── #
def get_news_content(news_url):
    try:
        r = _http_get(news_url, timeout=30)
    except RequestException as e:
        # erro de rede ou desconexão — sinaliza ao chamador
        raise Exception("falha de conexão") from e

    if r.status_code != 200:
        raise Exception(f"status {r.status_code}")
    soup = BeautifulSoup(r.text, 'html.parser')
    title = soup.title.text.strip()

    cfg = get_config_for_site(news_url)
    # localiza content_div
    selector = cfg["content_selector"].split(',')
    content_div = None
    for sel in selector:
        typ = sel.strip().split('.')[0]
        cls = sel.strip().split('.')[1] if '.' in sel else None
        content_div = soup.find(typ, class_=cls) if cls else soup.find(typ)
        if content_div: break
    if not content_div:
        # tentativa genérica antes de desistir
        content_div = soup.find('article') or soup.find('main') or soup

    paragraphs = content_div.find_all(cfg["paragraph_selector"]) or content_div.find_all("p")
    if not paragraphs:
        raise Exception("conteúdo não encontrado (parágrafos vazios)")
    # imgs internas
    imgs = (content_div.find_all("figure")
            if cfg["img_selector"] == "figure"
            else content_div.find_all("img"))
    # ───────── thumbnail
    thumb = None
    for sel in cfg["thumbnail_selector"].split(','):
        typ = sel.strip().split('.')[0]
        cls = sel.strip().split('.')[1] if '.' in sel else None
        thumb = soup.find(typ, class_=cls) if cls else soup.find(typ)
        if thumb: break
    if thumb and cfg["thumbnail_img_selector"] == "img":
        thumb_img = thumb.find("img")
    else:
        thumb_img = thumb
    thumbnail_url = (thumb_img.get('src') or thumb_img.get('data-src')
                     or thumb_img.get('data-lazy-src')) if thumb_img else None
    # ignora thumbnails data-URI
    if thumbnail_url and thumbnail_url.startswith("data:"):
        thumbnail_url = None
    # fallback og:image
    if not thumbnail_url:
        og = soup.find("meta", property="og:image")
        thumbnail_url = og.get("content") if og else None
        if not thumbnail_url:
            # primeira imagem externa encontrada
            for im in soup.find_all("img"):
                cand = im.get('src') or im.get('data-src') or im.get('data-lazy-src')
                if cand and cand.startswith("http"):
                    thumbnail_url = cand
                    break
    # garante que a thumbnail é URL absoluta
    if thumbnail_url:
        thumbnail_url = urljoin(news_url, thumbnail_url)
    if not thumbnail_url:
        raise Exception("thumbnail não encontrada")

    # texto
    first_idx = next((i for i,p in enumerate(paragraphs) if p.text.strip()),0)
    excerpt = paragraphs[first_idx].text.strip()
    content = '\n'.join(p.text for p in paragraphs[first_idx:]
                        if not re.search(r'RELATED|READ\s+MORE|NEXT', p.text, re.I))

    # processa imagens
    uploads = []
    for el in imgs:
        img_tag = el.find('img') if el.name == 'figure' else el
        src = (img_tag.get('src') or img_tag.get('data-src')
               or img_tag.get('data-lazy-src')) if img_tag else None
        # converte para URL absoluta
        if src:
            src = urljoin(news_url, src)
        if not src or src == thumbnail_url:
            continue
        fname = os.path.basename(src.split('?')[0])
        download_image(src, fname)
        up_id = upload_image(fname)
        if up_id: uploads.append(up_id)

    return title, excerpt, content, thumbnail_url, uploads

# ───────────────────────────── LISTA DE FEEDS ──────────────────────────── #
rss_feed_urls = [
    # internacionais
    "https://boundingintocomics.com/feed/",
    "https://thatparkplace.com/feed/",
    "http://feeds.ign.com/ign/all",
    "https://www.polygon.com/rss/index.xml",
    "https://www.cbr.com/feed/",
    # brasileiros – cinema, games e HQ
    "https://jovemnerd.com.br/feed/",
    "https://www.omelete.com.br/rss.xml",
    "https://www.adorocinema.com/rss/noticias.xml",
    "https://cinepop.com.br/feed/",
    "https://observatoriodegames.uol.com.br/feed"
]

# URLs que não possuem corpo de artigo (listas de compras, pré-venda, promoções etc.)
IGNORE_URL_PATTERNS = [
    r"/pre-order/",
    r"/deal[s]?/",
    r"/deals?/",
    r"/list/",
]

# ───────────────────────────── LOOP PRINCIPAL ─────────────────────────── #
categories = get_all_categories()

def process_news_feed(feed_url):
    try:
        feed = feedparser.parse(
            _http_get(feed_url, timeout=30).content
        )
        for entry in feed.entries:
            news_url = entry.link
            # ── DEBUG: explica por que determinados links são ignorados ── #
            reason = None
            if re.search(r"(podcast|nerdcast|youtube\.com|/video/|/videos?/)", news_url, re.I):
                reason = "podcast/vídeo"
            elif any(re.search(pat, news_url, re.I) for pat in IGNORE_URL_PATTERNS):
                reason = "pré‑venda/lista"
            elif is_url_posted(news_url):
                reason = "já postado"
            if reason:
                print("· SKIP", reason, "—", news_url)
                continue
            print("→", news_url)
            try:
                title, excerpt, content, thumb_url, imgs_ids = get_news_content(news_url)
                title = clean_site_name_from_title(title)

                tr_title   = translate_content(title,   title_prompt)
                tr_excerpt = translate_content(excerpt, excerpt_prompt)
                tr_content = translate_content(content, content_prompt)
                html_body  = text_to_html_paragraphs(tr_content)
                html_body  = insere_imagens_apos_h2(html_body, imgs_ids)

                # thumbnail tmp file
                thumb_path = "thumb_tmp.jpg"
                download_image(thumb_url, thumb_path)

                cat_ids = match_categories(tr_title + " " + tr_content, categories)
                if not cat_ids:
                    cat_ids = [get_or_create_category_id("Notícias")]

                if post_to_wordpress(tr_title, html_body, tr_excerpt,
                                     thumb_path, cat_ids):
                    log_posted(news_url)
            except Exception as e:
                msg = str(e).lower()
                if "conteúdo não encontrado" in msg:
                    # página sem corpo de artigo — ignora em silêncio
                    print("Conteúdo não encontrado – pulando.")
                elif "rate limit" in msg:
                    # excedeu cota da API — apenas registra e segue
                    print("Rate-limit da API – pulando este item.")
                elif "falha de conexão" in msg:
                    print("Falha de conexão – pulando este item.")
                else:
                    print("Erro item:", e)
                    traceback.print_exc()
    except Exception as e:
        print("Erro feed:", feed_url, e)

for url in rss_feed_urls:
    print(f"\n=== PROCESSANDO FEED: {url} ===")
    process_news_feed(url)
